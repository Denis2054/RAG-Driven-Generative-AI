{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpsYn49QWSR"
      },
      "source": [
        "#Introducing Naïve, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "It explores keyword matching, vector search, and index-based retrieval methods. Using OpenAI's GPT models, it generates responses based on input queries and retrieved documents.\n",
        "\n",
        "The modular RAG system offers flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "- Introduction of Naïve, Advanced, and Modular RAG\n",
        "- Environment setup for OpenAI API integration\n",
        "- Generator function using GPT models\n",
        "- Formatted response printing\n",
        "- **Data** setup with a list of documents (db_records)\n",
        "- **Query** user request\n",
        "- **1.Naïve RAG**:\n",
        "  - Keyword search and matching function\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT\n",
        "- **2.Advanced RAG**:\n",
        "  - Vector search:\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "  - Index-based retrieval:\n",
        "    - Setup of TF-IDF vectorizer and matrix\n",
        "    - Cosine similarity calculation\n",
        "    - Augmented input creation\n",
        "    - Generation with GPT\n",
        "- **3.Modular RAG Retriever**:\n",
        "  - RetrievalComponent class with methods for keyword, vector, and indexed search\n",
        "  - Usage example with different retrieval methods\n",
        "  - Augmented input creation\n",
        "  - Generation with GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o01-IM8bTc5f"
      },
      "source": [
        "# The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCfbN0YwHbE",
        "outputId": "88384b2c-1f4c-4d09-ce8c-c42e2866170b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.19.0 in /usr/local/lib/python3.10/dist-packages (1.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.19.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.19.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.19.0) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.19.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.19.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.19.0) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myXJn33zbqTR",
        "outputId": "42c70c96-de12-49fd-d14f-1438ec469dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "outputs": [],
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      },
      "source": [
        "# The Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time()  # Start timing before the request\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      },
      "source": [
        "### Formatted response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      },
      "source": [
        " # The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "outputs": [],
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "FIQ7NK92g7EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc6e26d-b2c4-44cc-92fc-0293adb72241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      },
      "source": [
        "## The Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "outputs": [],
      "source": [
        "query = \"define a rag store\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4lLAvV6NIn_"
      },
      "source": [
        "### Generation without augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "iO-k1Tq4MqmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0679f2d6-085c-424b-b6e3-78b6bd09cd14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! The content you've provided appears to be a sequence of characters\n",
            "that, when combined, spell out the phrase \"define a rag store.\" Let's break this\n",
            "down step by step:  1. **Character Sequence**:    - The sequence of characters\n",
            "is: `d e f i n e   a   r a g   s t o r e`.    - When these characters are\n",
            "concatenated, they form the phrase: \"define a rag store\".  2. **Phrase\n",
            "Breakdown**:    - **Define**: This is a verb that means to explain the meaning\n",
            "of a word or concept.    - **A**: This is an indefinite article used before\n",
            "words that begin with a consonant sound.    - **Rag**: This is a noun that\n",
            "typically refers to a piece of old cloth, often used for cleaning.    -\n",
            "**Store**: This is a noun that refers to a place where goods are sold.  3.\n",
            "**Combined Meaning**:    - When put together, the phrase \"define a rag store\" is\n",
            "asking for an explanation or definition of what a \"rag store\" is.  4. **Possible\n",
            "Interpretation**:    - A \"rag store\" could be interpreted as a shop or store\n",
            "that specializes in selling rags or old cloths. These types of stores might sell\n",
            "cleaning rags, recycled fabric, or other textile materials that are no longer in\n",
            "their original form but can be repurposed.  5. **Contextual Usage**:    - The\n",
            "phrase could be used in various contexts, such as:      - A dictionary or\n",
            "glossary entry explaining what a rag store is.      - A conversation where\n",
            "someone is asking for clarification about the term.      - An academic or\n",
            "business setting where the concept of a rag store is being discussed.  By\n",
            "breaking down the sequence of characters and understanding the individual\n",
            "components, we can derive the meaning and context of the phrase \"define a rag\n",
            "store.\"\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqh9rr81SUn"
      },
      "source": [
        "# 1.Naïve Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8vteKmS_qO"
      },
      "source": [
        "## Keyword search and matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WY1JU0Ush_l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d194a6-5430-4d5b-97c8-3a758c99b620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 3\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      },
      "source": [
        "## Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \": \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7NTfxzum0PT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a9914b-4065-4a17-8262-da4bc232a940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Jh8BsnUy0Fcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e5460f-a0a3-4f4a-c48c-c524d8f0dac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the provided content:  ### Define a\n",
            "RAG Store:  A **RAG (Retrieval-Augmented Generation) vector store** is a\n",
            "specialized type of database or dataset that is designed to store and manage\n",
            "vectorized data points.   #### Key Concepts:  1. **Vectorized Data Points**:\n",
            "- **Vectors**: In the context of machine learning and natural language\n",
            "processing, vectors are numerical representations of data. For example, words,\n",
            "sentences, or documents can be converted into vectors using various embedding\n",
            "techniques like Word2Vec, GloVe, or BERT.    - **Vectorization**: This is the\n",
            "process of converting data into a numerical format that can be easily processed\n",
            "by machine learning algorithms. For text data, this often involves transforming\n",
            "words or phrases into high-dimensional vectors that capture semantic meaning.\n",
            "2. **RAG (Retrieval-Augmented Generation)**:    - **Retrieval**: This involves\n",
            "fetching relevant information from a large dataset or database. In the context\n",
            "of a RAG store, this means retrieving the most relevant vectorized data points\n",
            "based on a query.    - **Augmented Generation**: This refers to the process of\n",
            "generating new content or responses by leveraging the retrieved information. For\n",
            "example, a language model might use the retrieved vectors to generate a coherent\n",
            "and contextually appropriate response.  3. **Vector Store**:    - **Database**:\n",
            "A structured collection of data that allows for efficient storage, retrieval,\n",
            "and management of vectorized data points. This can be implemented using various\n",
            "database technologies optimized for handling high-dimensional vectors.    -\n",
            "**Dataset**: A collection of vectorized data points that can be used for\n",
            "training, testing, or deploying machine learning models.  #### Applications:  -\n",
            "**Information Retrieval**: Quickly finding relevant documents or data points\n",
            "based on a query. - **Question Answering Systems**: Enhancing the ability of AI\n",
            "models to provide accurate and contextually relevant answers by retrieving\n",
            "pertinent information. - **Recommendation Systems**: Suggesting items or content\n",
            "based on the similarity of vectorized representations.  #### Example Workflow:\n",
            "1. **Data Vectorization**: Convert raw data (e.g., text, images) into vectors\n",
            "using an embedding model. 2. **Storage**: Store these vectors in a RAG vector\n",
            "store. 3. **Querying**: When a query is made, retrieve the most relevant vectors\n",
            "from the store. 4. **Augmentation**: Use the retrieved vectors to generate or\n",
            "enhance the response.  In summary, a RAG vector store is a powerful tool for\n",
            "managing and utilizing vectorized data points, enabling advanced capabilities in\n",
            "information retrieval and content generation.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH2__0iTUr1"
      },
      "source": [
        "# 2.Advanced Retrieval Augmented Generation(RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyjcn35jFiy"
      },
      "source": [
        "## 2.1.Vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "FCBbY4qLc8qh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ],
      "metadata": {
        "id": "RG1iM-U33OCg",
        "outputId": "b0eb82c5-7c06-465d-8810-5b4f4e7e7966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.215\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fZC6Oe2G9E"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "T3uk-91x049J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9373b7-a65e-41b5-ad9e-bdf0a53c3146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFDF6hbi2LF9"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "YJC-mA5ftxFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828facf8-f547-4ebd-f53e-850f023758a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the provided content:  **Define a\n",
            "RAG store:**  A **RAG vector store** is a database or dataset that contains\n",
            "vectorized data points.  ### Explanation:  1. **RAG**:    - RAG stands for\n",
            "**Retrieval-Augmented Generation**. This is a technique used in natural language\n",
            "processing (NLP) where a model retrieves relevant information from a database or\n",
            "dataset to augment its generation capabilities. This helps in providing more\n",
            "accurate and contextually relevant responses.  2. **Vector Store**:    - A\n",
            "vector store is a specialized type of database designed to store and manage\n",
            "vectorized data points. In the context of NLP and machine learning, data points\n",
            "are often transformed into vectors (numerical representations) to facilitate\n",
            "various computational processes such as similarity search, clustering, and\n",
            "classification.  3. **Vectorized Data Points**:    - Vectorized data points are\n",
            "numerical representations of data. For example, in NLP, words, sentences, or\n",
            "documents can be converted into vectors using techniques like word embeddings\n",
            "(e.g., Word2Vec, GloVe) or contextual embeddings (e.g., BERT, GPT). These\n",
            "vectors capture the semantic meaning and relationships between the data points.\n",
            "### Putting It All Together:  A **RAG vector store** is essentially a database\n",
            "that holds these vectorized representations of data. When a model needs to\n",
            "generate a response or perform a task, it can retrieve relevant vectors from\n",
            "this store to enhance its performance. This retrieval step ensures that the\n",
            "model has access to pertinent information, making its outputs more accurate and\n",
            "contextually appropriate.  ### Example Use Case:  Imagine you have a chatbot\n",
            "designed to answer questions about a large collection of documents. Instead of\n",
            "relying solely on its pre-trained knowledge, the chatbot can use a RAG vector\n",
            "store to retrieve relevant document vectors based on the user's query. By\n",
            "incorporating this retrieved information, the chatbot can provide more precise\n",
            "and informative answers.  In summary, a RAG vector store is a powerful tool in\n",
            "the realm of NLP, enabling models to leverage external data effectively through\n",
            "vectorized representations.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7djpPBpm0M2"
      },
      "source": [
        "## 2.2.Index-based search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "wRarT_fym2XC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fea49e4-bc54-461b-9a0d-4491f883f439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "SbokQ2eacHjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a54c78e-82b7-4f29-81a0-88b89a61b632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
            "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "\n",
            "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
            "\n",
            "       where     which    while     wide      with    within   without  \n",
            "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
            "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
            "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[28 rows x 297 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABZ12Bkugtt"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MNozI65K4e7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ea0008-5173-455e-dbf9-1b5fba78afb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU998zkD4hpD"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "uy9X-l_Iugtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77156ee-2f66-4129-9206-2f2110b7f364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the content provided:  ### Define a\n",
            "RAG Store:  A **RAG (Retrieval-Augmented Generation)** vector store is a\n",
            "specialized type of database or dataset that is designed to store and manage\n",
            "vectorized data points.   #### Key Concepts:  1. **Vectorized Data Points**:\n",
            "- **Vectorization**: This is the process of converting data into a numerical\n",
            "format that can be easily processed by machine learning algorithms. In the\n",
            "context of text data, this often involves transforming words, sentences, or\n",
            "documents into vectors (arrays of numbers) that capture the semantic meaning of\n",
            "the text.    - **Data Points**: These are individual pieces of data that have\n",
            "been vectorized. For example, each data point could be a vector representation\n",
            "of a sentence or a document.  2. **RAG (Retrieval-Augmented Generation)**:    -\n",
            "**Retrieval**: This involves searching and retrieving relevant data points from\n",
            "the vector store based on a query. The retrieval process typically uses\n",
            "similarity measures to find vectors that are close to the query vector in the\n",
            "vector space.    - **Augmented Generation**: After retrieving relevant data\n",
            "points, these are used to augment the generation process. For example, in\n",
            "natural language processing, retrieved vectors can be used to provide context or\n",
            "additional information to a language model, improving the quality and relevance\n",
            "of the generated text.  3. **Vector Store**:    - **Database or Dataset**: A\n",
            "vector store can be implemented as a database or a dataset that is optimized for\n",
            "storing and querying vectorized data. This often involves specialized indexing\n",
            "techniques to enable efficient similarity searches.    - **Contains Vectorized\n",
            "Data Points**: The primary content of a vector store is the vectorized\n",
            "representations of data points. These vectors are typically high-dimensional and\n",
            "capture various features of the original data.  #### Applications:  - **Natural\n",
            "Language Processing (NLP)**: In NLP, a RAG vector store can be used to enhance\n",
            "the capabilities of language models by providing them with relevant context\n",
            "retrieved from the vector store. - **Recommendation Systems**: Vector stores can\n",
            "be used to store user preferences and item features, enabling efficient\n",
            "retrieval of similar items for recommendations. - **Information Retrieval**:\n",
            "Vector stores can be used to improve search engines by enabling semantic search,\n",
            "where the search results are based on the meaning of the query rather than just\n",
            "keyword matching.  #### Example:  Imagine you have a large collection of\n",
            "documents, and you want to build a system that can answer questions based on the\n",
            "content of these documents. You can:  1. **Vectorize** each document into a\n",
            "numerical representation. 2. Store these vectors in a RAG vector store. 3. When\n",
            "a user asks a question, convert the question into a vector. 4. Retrieve the most\n",
            "relevant document vectors from the vector store. 5. Use these retrieved vectors\n",
            "to generate a coherent and relevant answer to the user's question.  In summary,\n",
            "a RAG vector store is a powerful tool for managing and utilizing vectorized data\n",
            "points, enabling advanced applications in various fields such as NLP,\n",
            "recommendation systems, and information retrieval.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEvzcDHTX6i"
      },
      "source": [
        "# 3.Modular RAG Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHm4saJ8cGk"
      },
      "source": [
        "### Modular RAG Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "_kvhIOdY8amp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa0a0d1-9537-44cb-c8e3-23ec454b1067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "#print(f\"Best Matching Record: {best_matching_record}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      },
      "source": [
        "### Augmented Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \" \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ZhSO-fyZ7Jwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72844538-e370-4a72-bbdf-f3e422d842cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "-V3srRHW7Jwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1908f0f4-ebdf-465a-84fb-49f67377e925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down and elaborate on the provided content:  ---\n",
            "**Define a RAG store:**  A **RAG (Retrieval-Augmented Generation)** store is a\n",
            "specialized type of data storage system designed to support the RAG framework.\n",
            "This framework combines the strengths of retrieval-based and generation-based\n",
            "models to improve the performance of natural language processing tasks.  **ARA\n",
            "(Approximate Nearest Neighbor) vector store:**  An **ARA vector store** is a\n",
            "database or dataset that contains vectorized data points. These data points are\n",
            "typically high-dimensional vectors that represent various forms of data, such as\n",
            "text, images, or other types of information. The purpose of vectorizing data is\n",
            "to transform it into a numerical format that can be efficiently processed and\n",
            "analyzed by machine learning algorithms.  **Vector store:**  A **vector store**\n",
            "is a type of database specifically designed to store and manage these high-\n",
            "dimensional vectors. It allows for efficient storage, retrieval, and\n",
            "manipulation of vectorized data points. Vector stores are particularly useful in\n",
            "applications such as:  1. **Similarity Search:** Finding data points that are\n",
            "similar to a given query vector. 2. **Clustering:** Grouping similar data points\n",
            "together based on their vector representations. 3. **Classification:** Assigning\n",
            "labels to data points based on their vector representations. 4. **Recommendation\n",
            "Systems:** Suggesting items to users based on the similarity of their vector\n",
            "representations.  **Vectorized data points:**  **Vectorized data points** are\n",
            "numerical representations of data that have been transformed into vectors. This\n",
            "transformation process is known as vectorization and is a crucial step in many\n",
            "machine learning and data analysis workflows. For example:  - In natural\n",
            "language processing, text data can be vectorized using techniques like word\n",
            "embeddings (e.g., Word2Vec, GloVe) or sentence embeddings (e.g., BERT, GPT). -\n",
            "In image processing, images can be vectorized using convolutional neural\n",
            "networks (CNNs) to extract feature vectors.  By storing these vectorized data\n",
            "points in a vector store, it becomes possible to perform various operations on\n",
            "the data efficiently, such as searching for similar items, clustering, and\n",
            "classification.  ---  In summary, a RAG store is a database or dataset that\n",
            "contains vectorized data points, which are numerical representations of various\n",
            "forms of data. These vectorized data points are stored in a vector store,\n",
            "allowing for efficient retrieval and manipulation of the data for various\n",
            "machine learning and data analysis tasks.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFY89PqkAZWWUfEdHusgVP"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}